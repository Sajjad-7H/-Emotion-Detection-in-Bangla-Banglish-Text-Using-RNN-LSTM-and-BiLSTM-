# -*- coding: utf-8 -*-
"""nnfl_project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NP7AC7BQRUi34IEOCX2nKe5D1rKx3jLv
"""

import pandas as pd
import numpy as np
import re
import unicodedata
from sklearn.model_selection import train_test_split
from collections import Counter

file_path = "Bengali_Banglish_80K_Dataset.csv"
df = pd.read_csv(file_path)

print("Dataset loaded successfully!")
print("Shape:", df.shape)
print(df.head())

print("\nMissing values:\n", df.isnull().sum())
print("\nUnique emotions:", df['Label'].unique())
print("\nEmotion distribution:")
print(df['Label'].value_counts())

def detect_script(text):
    for ch in str(text):
        if '\u0980' <= ch <= '\u09FF':
            return 'bengali'
    return 'banglish'

df['script'] = df['Bengali'].apply(detect_script)
print("\nScript counts:")
print(df['script'].value_counts())

def clean_text(text):
    text = str(text)
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#(\w+)', r'\1', text)
    text = re.sub(r'[^0-9A-Za-z\u0980-\u09FF!?.,\'" ]+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.lower()

df['clean_bengali'] = df['Bengali'].apply(clean_text)
df['clean_banglish'] = df['Banglish'].fillna('').apply(clean_text)

df.drop_duplicates(subset='clean_bengali', inplace=True)
df = df[df['clean_bengali'].str.len() > 2]
df.reset_index(drop=True, inplace=True)
print("\nAfter cleaning:", df.shape)

emotion_list = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']
label_map = {emo: i for i, emo in enumerate(emotion_list)}
df.loc[:, 'label'] = df['Label'].map(label_map)


print("\nLabel mapping:", label_map)
print(df[['Label', 'label']].head())

train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)

print(f"\nTrain: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}")

train_df.to_csv("train_clean.csv", index=False)
val_df.to_csv("val_clean.csv", index=False)
test_df.to_csv("test_clean.csv", index=False)

print("\n Preprocessing complete! Cleaned files saved:")
print(" - train_clean.csv")
print(" - val_clean.csv")
print(" - test_clean.csv")

print("\nClass distribution in training set:")
print(train_df['label'].value_counts())

print("\nAverage text length (in characters):")
print(train_df['clean_bengali'].str.len().mean())

print("\nClass distribution in training set:")
print(train_df['label'].value_counts())

print("\nAverage text length (in characters):")
print(train_df['clean_bengali'].str.len().mean())

print("\nScript counts:")
print(df['script'].value_counts())

train_df = pd.read_csv("train_clean.csv")
val_df   = pd.read_csv("val_clean.csv")
test_df  = pd.read_csv("test_clean.csv")

print(train_df.shape, val_df.shape, test_df.shape)

vocab_size = 50000
max_len    = 80

def tokenize(text):
    return str(text).lower().strip().split()

from collections import Counter

all_words = []
for t in train_df['clean_bengali']:
    all_words.extend(tokenize(t))

freq = Counter(all_words)

most_common = freq.most_common(vocab_size - 2)

word2id = {"<PAD>": 0, "<UNK>": 1}

for i, (word, count) in enumerate(most_common, start=2):
    word2id[word] = i


id2word = {v: k for k, v in word2id.items()}

def text_to_ids(text):
    tokens = tokenize(text)
    ids = [word2id.get(token, 1) for token in tokens]


    if len(ids) < max_len:
        ids += [0] * (max_len - len(ids))
    else:
        ids = ids[:max_len]

    return ids

def df_to_array(df):
    X = np.array([text_to_ids(t) for t in df["clean_bengali"]])
    y = df["label"].values
    return X, y

train_X, train_y = df_to_array(train_df)
val_X,   val_y   = df_to_array(val_df)
test_X,  test_y  = df_to_array(test_df)

import json
import os

os.makedirs("project/artifacts", exist_ok=True)


with open("project/artifacts/vocab.json", "w") as f:
    json.dump(word2id, f)


np.save("project/artifacts/train_X.npy", train_X)
np.save("project/artifacts/val_X.npy", val_X)
np.save("project/artifacts/test_X.npy", test_X)

np.save("project/artifacts/train_y.npy", train_y)
np.save("project/artifacts/val_y.npy", val_y)
np.save("project/artifacts/test_y.npy", test_y)

print("Tokenization complete and artifacts saved!")

train_X, train_y
val_X, val_y
test_X, test_y
word2id

import numpy as np
import json

train_X = np.load("project/artifacts/train_X.npy")
val_X   = np.load("project/artifacts/val_X.npy")
test_X  = np.load("project/artifacts/test_X.npy")

train_y = np.load("project/artifacts/train_y.npy")
val_y   = np.load("project/artifacts/val_y.npy")
test_y  = np.load("project/artifacts/test_y.npy")

with open("project/artifacts/vocab.json") as f:
    word2id = json.load(f)

num_classes = 6
vocab_size  = len(word2id)
max_len     = train_X.shape[1]
print("Shapes:", train_X.shape, val_X.shape, test_X.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, f1_score

import numpy as np

emotion_names = ["anger","disgust","fear","joy","sadness","surprise"]

def train_once(model, X_tr, y_tr, X_val, y_val,
               lr, batch_size, epochs):
    """Compile + train a model once with fixed hyperparameters."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )

    early = EarlyStopping(
        monitor="val_loss",
        patience=2,
        restore_best_weights=True
    )
    reduce = ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=1,
        min_lr=1e-5
    )

    hist = model.fit(
        X_tr, y_tr,
        validation_data=(X_val, y_val),
        batch_size=batch_size,
        epochs=epochs,
        callbacks=[early, reduce],
        verbose=2
    )
    return hist

def plot_acc_curve(history, title):
    """Train vs validation accuracy curve."""
    plt.figure(figsize=(7,5))
    plt.plot(history.history["accuracy"],     label="train", marker="o")
    plt.plot(history.history["val_accuracy"], label="val",   marker="o")
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.grid(True)
    plt.legend()
    plt.show()

def full_eval(model, X, y, batch_size, model_name):
    """Accuracy, macro F1, confusion matrix, and per-class metrics."""
    prob = model.predict(X, batch_size=batch_size, verbose=0)
    pred = prob.argmax(axis=1)

    acc = (pred == y).mean()
    m_f1 = f1_score(y, pred, average="macro")

    print(f"\n[{model_name}]")
    print(f"Accuracy   : {acc:.4f}")
    print(f"Macro F1   : {m_f1:.4f}")
    print("\nClassification report:")
    print(classification_report(y, pred, target_names=emotion_names))

    cm = confusion_matrix(y, pred)
    plt.figure(figsize=(6,5))
    plt.imshow(cm, cmap="Blues")
    plt.colorbar()
    plt.xticks(ticks=range(len(emotion_names)), labels=emotion_names, rotation=45)
    plt.yticks(ticks=range(len(emotion_names)), labels=emotion_names)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"Confusion Matrix – {model_name}")
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j], ha="center", va="center", color="black")
    plt.tight_layout()
    plt.show()


    report = classification_report(y, pred, target_names=emotion_names, output_dict=True)
    f1_values = [report[e]["f1-score"] for e in emotion_names]
    plt.figure(figsize=(7,4))
    plt.bar(emotion_names, f1_values)
    plt.ylim(0, 1)
    plt.title(f"Per-class F1 – {model_name}")
    plt.ylabel("F1-score")
    plt.grid(axis="y")
    plt.show()

    return acc, m_f1

def build_lstm(vocab_size, emb_dim, lstm_units, dropout_rate, n_classes):
    net = Sequential()
    net.add(Embedding(input_dim=vocab_size, output_dim=emb_dim))

    net.add(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=0.0))
    net.add(Dense(128, activation="relu"))
    net.add(Dropout(dropout_rate))
    net.add(Dense(n_classes, activation="softmax"))
    return net

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import numpy as np
import json

def train_once(model, X_tr, y_tr, X_val, y_val,
               lr, batch_size, epochs):
    """Compile + train a model once with fixed hyperparameters."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )

    early = EarlyStopping(
        monitor="val_loss",
        patience=2,
        restore_best_weights=True
    )
    reduce = ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=1,
        min_lr=1e-5
    )

    hist = model.fit(
        X_tr, y_tr,
        validation_data=(X_val, y_val),
        batch_size=batch_size,
        epochs=epochs,
        callbacks=[early, reduce],
        verbose=2
    )
    return hist

def build_lstm(vocab_size, emb_dim, lstm_units, dropout_rate, n_classes):
    net = Sequential()
    net.add(Embedding(input_dim=vocab_size, output_dim=emb_dim))

    net.add(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=0.0))
    net.add(Dense(128, activation="relu"))
    net.add(Dropout(dropout_rate))
    net.add(Dense(n_classes, activation="softmax"))
    return net


train_X = np.load("project/artifacts/train_X.npy")
val_X   = np.load("project/artifacts/val_X.npy")
test_X  = np.load("project/artifacts/test_X.npy")

train_y = np.load("project/artifacts/train_y.npy")
val_y   = np.load("project/artifacts/val_y.npy")
test_y  = np.load("project/artifacts/test_y.npy")

with open("project/artifacts/vocab.json", "r") as f:
    word2id = json.load(f)

num_classes = 6
vocab_size  = len(word2id)
max_len     = train_X.shape[1]

cfg_lstm_1 = {
    "emb_dim":   100,
    "units":     128,
    "drop":      0.30,
    "lr":        0.003,
    "batch":     256,
    "epochs":    12
}

model_lstm_1 = build_lstm(
    vocab_size=vocab_size,
    emb_dim=cfg_lstm_1["emb_dim"],
    lstm_units=cfg_lstm_1["units"],
    dropout_rate=cfg_lstm_1["drop"],
    n_classes=num_classes
)

history_lstm_1 = train_once(
    model_lstm_1,
    train_X, train_y,
    val_X,   val_y,
    lr=cfg_lstm_1["lr"],
    batch_size=cfg_lstm_1["batch"],
    epochs=cfg_lstm_1["epochs"]
)

cfg_lstm_1 = {
    "emb_dim":   100,
    "units":     128,
    "drop":      0.30,
    "lr":        0.003,
    "batch":     256,
    "epochs":    12
}

model_lstm_1 = build_lstm(
    vocab_size=vocab_size,
    emb_dim=cfg_lstm_1["emb_dim"],
    lstm_units=cfg_lstm_1["units"],
    dropout_rate=cfg_lstm_1["drop"],
    n_classes=num_classes
)
model_lstm_1.summary()

acc_lstm_1, f1_lstm_1 = full_eval(
    model_lstm_1,
    test_X, test_y,
    batch_size=cfg_lstm_1["batch"],
    model_name="LSTM – Run 1"
)

plot_acc_curve(history_lstm_1, "LSTM – Run 1 (baseline)")

train_acc_1 = history_lstm_1.history["accuracy"][-1]
val_acc_1   = history_lstm_1.history["val_accuracy"][-1]
print(f"Train acc: {train_acc_1:.3f} | Val acc: {val_acc_1:.3f}")

if train_acc_1 - val_acc_1 > 0.10:
    print("Possible overfitting. (Gap > 0.10)")
    print('Prompt suggestion: "How can I reduce overfitting for sentiment analysis task?"')
else:
    print("No strong overfitting (gap ≤ 0.10).")

history_lstm_2 = train_once(
    model_lstm_2,
    train_X, train_y,
    val_X,   val_y,
    lr=cfg_lstm_2["lr"],
    batch_size=cfg_lstm_2["batch"],
    epochs=cfg_lstm_2["epochs"]
)

cfg_lstm_2 = {
    "emb_dim":   128,
    "units":     256,
    "drop":      0.40,
    "lr":        0.001,
    "batch":     256,
    "epochs":    15
}

model_lstm_2 = build_lstm(
    vocab_size=vocab_size,
    emb_dim=cfg_lstm_2["emb_dim"],
    lstm_units=cfg_lstm_2["units"],
    dropout_rate=cfg_lstm_2["drop"],
    n_classes=num_classes
)
model_lstm_2.summary()

plot_acc_curve(history_lstm_2, "LSTM – Run 2 (tuned)")

train_acc_2 = history_lstm_2.history["accuracy"][-1]
val_acc_2   = history_lstm_2.history["val_accuracy"][-1]
print(f"Train acc: {train_acc_2:.3f} | Val acc: {val_acc_2:.3f}")

if train_acc_2 - val_acc_2 > 0.10:
    print(" Possible overfitting. (Gap > 0.10)")
else:
    print("No strong overfitting (gap ≤ 0.10).")

acc_lstm_2, f1_lstm_2 = full_eval(
    model_lstm_2,
    test_X, test_y,
    batch_size=cfg_lstm_2["batch"],
    model_name="LSTM – Run 2"
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout

def build_bilstm(vocab_size, emb_dim, lstm_units, dropout_rate, n_classes):
    model = Sequential()


    model.add(Embedding(input_dim=vocab_size, output_dim=emb_dim))


    model.add(
        Bidirectional(
            LSTM(
                lstm_units,
                dropout=dropout_rate,
                recurrent_dropout=0.0
            )
        )
    )


    model.add(Dense(128, activation="relu"))
    model.add(Dropout(dropout_rate))


    model.add(Dense(n_classes, activation="softmax"))
    return model

cfg_bilstm_1 = {
    "emb_dim": 100,
    "units":   128,
    "drop":    0.30,
    "lr":      0.003,
    "batch":   256,
    "epochs":  12
}

bilstm_1 = build_bilstm(
    vocab_size=vocab_size,
    emb_dim=cfg_bilstm_1["emb_dim"],
    lstm_units=cfg_bilstm_1["units"],
    dropout_rate=cfg_bilstm_1["drop"],
    n_classes=num_classes
)
bilstm_1.summary()

hist_bilstm_1 = train_once(
    bilstm_1,
    train_X, train_y,
    val_X,   val_y,
    lr=cfg_bilstm_1["lr"],
    batch_size=cfg_bilstm_1["batch"],
    epochs=cfg_bilstm_1["epochs"]
)

plot_acc_curve(hist_bilstm_1, "BiLSTM – Run 1 (baseline)")

tr1 = hist_bilstm_1.history["accuracy"][-1]
va1 = hist_bilstm_1.history["val_accuracy"][-1]
print(f"Train acc: {tr1:.3f} | Val acc: {va1:.3f}")

if tr1 - va1 > 0.10:
    print(" Possible overfitting (gap > 0.10).")
    print('Prompt idea: "How can I reduce overfitting for sentiment analysis task?"')
else:
    print("No strong overfitting (gap ≤ 0.10).")

acc_bi_1, f1_bi_1 = full_eval(
    bilstm_1,
    test_X, test_y,
    batch_size=cfg_bilstm_1["batch"],
    model_name="BiLSTM – Run 1"
)

cfg_bilstm_2 = {
    "emb_dim": 128,
    "units":   256,
    "drop":    0.40,
    "lr":      0.001,
    "batch":   256,
    "epochs":  15
}

bilstm_2 = build_bilstm(
    vocab_size=vocab_size,
    emb_dim=cfg_bilstm_2["emb_dim"],
    lstm_units=cfg_bilstm_2["units"],
    dropout_rate=cfg_bilstm_2["drop"],
    n_classes=num_classes
)
bilstm_2.summary()

hist_bilstm_2 = train_once(
    bilstm_2,
    train_X, train_y,
    val_X,   val_y,
    lr=cfg_bilstm_2["lr"],
    batch_size=cfg_bilstm_2["batch"],
    epochs=cfg_bilstm_2["epochs"]
)

plot_acc_curve(hist_bilstm_2, "BiLSTM – Run 2 (tuned)")

tr2 = hist_bilstm_2.history["accuracy"][-1]
va2 = hist_bilstm_2.history["val_accuracy"][-1]
print(f"Train acc: {tr2:.3f} | Val acc: {va2:.3f}")

if tr2 - va2 > 0.10:
    print(" Possible overfitting (gap > 0.10).")
else:
    print("No strong overfitting (gap ≤ 0.10).")

acc_bi_2, f1_bi_2 = full_eval(
    bilstm_2,
    test_X, test_y,
    batch_size=cfg_bilstm_2["batch"],
    model_name="BiLSTM – Run 2"
)

results_table = []

def add_result(name, cfg, acc, f1):
    results_table.append({
        "Model":     name,
        "Emb_dim":   cfg["emb_dim"],
        "Units":     cfg["units"],
        "Dropout":   cfg["drop"],
        "LR":        cfg["lr"],
        "Batch":     cfg["batch"],
        "Accuracy":  acc,
        "Macro_F1":  f1
    })


add_result("BiLSTM – Run 1", cfg_bilstm_1, acc_bi_1, f1_bi_1)
add_result("BiLSTM – Run 2", cfg_bilstm_2, acc_bi_2, f1_bi_2)

import pandas as pd
df_results = pd.DataFrame(results_table)
df_results

add_result("BiLSTM – Run 1", cfg_bilstm_1, acc_bi_1, f1_bi_1)
add_result("BiLSTM – Run 2", cfg_bilstm_2, acc_bi_2, f1_bi_2)

results_table = []

def add_result(name, cfg, acc, f1):
    results_table.append({
        "Model":    name,
        "Emb_dim":  cfg["emb_dim"],
        "Units":    cfg["units"],
        "Dropout":  cfg["drop"],
        "LR":       cfg["lr"],
        "Batch":    cfg["batch"],
        "Accuracy": acc,
        "Macro_F1": f1
    })


add_result("LSTM – Run 1",  cfg_lstm_1,  acc_lstm_1,  f1_lstm_1)
add_result("LSTM – Run 2",  cfg_lstm_2,  acc_lstm_2,  f1_lstm_2)


add_result("BiLSTM – Run 1", cfg_bilstm_1, acc_bi_1, f1_bi_1)
add_result("BiLSTM – Run 2", cfg_bilstm_2, acc_bi_2, f1_bi_2)



import pandas as pd

df_results = pd.DataFrame(results_table)
df_results = df_results.sort_values(by="Macro_F1", ascending=False)

print("===== Final Model Comparison =====")
display(df_results)

df_results.to_csv("project/artifacts/model_results_comparison.csv", index=False)
print("Saved: project/artifacts/model_results_comparison.csv")

model_lstm_2.save("project/artifacts/best_lstm_model.h5")
bilstm_2.save("project/artifacts/best_bilstm_model.h5")

print("Saved best models.")

from tensorflow.keras.models import load_model

loaded_lstm = load_model("project/artifacts/best_lstm_model.h5")
loaded_bilstm = load_model("project/artifacts/best_bilstm_model.h5")







